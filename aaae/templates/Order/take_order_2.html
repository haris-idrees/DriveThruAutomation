{% extends 'base.html' %}

{% load static %}

{% block content %}

<link rel="stylesheet" href="{% static 'css/Order/take_order.css' %}">

<div class="content">
    <div id="status">Listening...</div>
    <div id="conversation">
        <p><strong>You:</strong> <span id="userInput">-</span></p>
        <p><strong>AI:</strong> <span id="aiResponse">-</span></p>
        <p id="timeTaken" style="font-weight: bold; color: blue;"></p>
    </div>
</div>

<script>
    const synth = window.speechSynthesis;
    let recognition;
    let isRecognizing = false;
    let isSpeaking = false;
    let currentTranscript = '';
    let silenceTimer;
    let startTime; // Variable to record the start time

    const statusText = document.getElementById("status");
    const userInputText = document.getElementById("userInput");
    const aiResponseText = document.getElementById("aiResponse");
    const timeTakenDisplay = document.getElementById("timeTaken");

    // Initialize Speech Recognition
    if ('SpeechRecognition' in window || 'webkitSpeechRecognition' in window) {
        recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        recognition.continuous = true;
        recognition.interimResults = true;

        recognition.onstart = function () {
            console.log("Recognition started...");
            isRecognizing = true;
        };

        recognition.onresult = function (event) {
            clearTimeout(silenceTimer);

            let transcript = '';
            for (let i = event.resultIndex; i < event.results.length; i++) {
                transcript += event.results[i][0].transcript;
            }

            console.log("Transcript captured:", transcript);
            userInputText.textContent = transcript;
            currentTranscript = transcript;

            if (event.results[event.resultIndex].isFinal) {
                sendToBackend(transcript);
            } else {
                // Start silence timer for interim results
                silenceTimer = setTimeout(() => {
                    console.log("Silence detected, sending interim transcript.");
                    sendToBackend(transcript);
                }, 1000); // 1 second of silence
            }
        };

        recognition.onerror = function (event) {
            console.error("Speech recognition error:", event.error);
            if (event.error !== 'no-speech') {
                recognition.stop();
                isRecognizing = false;
                statusText.textContent = "Listening...";
                startRecognition();
            }
        };

        recognition.onend = function () {
            console.log("Recognition ended...");
            isRecognizing = false;
            if (!isSpeaking) {
                startRecognition();
            }
        };
    } else {
        console.log("SpeechRecognition API is not supported.");
    }

    window.onload = function () {
        startRecognition();
    };

    // Start Speech Recognition
    function startRecognition() {
        if (isRecognizing) {
            console.log("Recognition already started");
            return;
        }
        recognition.start();
        isRecognizing = true;
        statusText.textContent = "Listening...";
    }

    // Send transcription to backend
    function sendToBackend(transcript) {
        console.log("Sending transcription to backend:", transcript);

        // Record the start time
        startTime = performance.now();

        fetch("{% url 'process_speech' %}", {
            method: "POST",
            headers: {
                "Content-Type": "application/json",
                "X-CSRFToken": "{{ csrf_token }}"
            },
            body: JSON.stringify({ transcript })
        })
        .then(response => response.json())
        .then(data => {
            if (data.response) {
                aiResponseText.textContent = data.response;
                speakResponse(data.response);
            }
        })
        .catch(error => {
            console.error("Error sending data to backend:", error);
        });
    }

    <script src="https://gist.github.com/guest271314/f48ee0658bc9b948766c67126ba9104c.js"></script>

    // Speak the response from the backend using SpeechSynthesisUtterance
    function speakResponse(responseText) {
<!--        const cleanedResponse = responseText.replace(/[*?#-]/g, "");-->
<!--        const utterance = new SpeechSynthesisUtterance(cleanedResponse);-->

<!--        const voices = synth.getVoices();-->

<!--        utterance.voice = voices[5];-->

<!--        isSpeaking = true;-->
<!--        statusText.textContent = "Speaking...";-->

<!--        recognition.stop();-->

<!--        console.log(responseText);-->

<!--        // Calculate and display the total time-->
<!--        const endTime = performance.now();-->
<!--        const totalTime = (endTime - startTime).toFixed(2); // Time in milliseconds-->
<!--        console.log(`Total time from sending to speaking: ${totalTime} ms`);-->
<!--        timeTakenDisplay.textContent = `Total Time: ${totalTime} ms`;-->

<!--        utterance.onend = function () {-->
<!--            isSpeaking = false;-->
<!--            console.log("Speech finished");-->

<!--            if (responseText.includes("[ORDER_CONFIRM]")) {-->
<!--                console.log("Order completed");-->
<!--                window.location.href = "{% url 'confirm_order' %}";-->
<!--            } else {-->
<!--                statusText.textContent = "Listening...";-->
<!--                startRecognition();-->
<!--            }-->
<!--        };-->

<!--        utterance.onerror = function (event) {-->
<!--            console.error("Error during speech synthesis:", event.error);-->
<!--            isSpeaking = false;-->
<!--            statusText.textContent = "Listening...";-->
<!--            startRecognition();-->
<!--        };-->

<!--        speechSynthesis.speak(utterance);-->

            meSpeak.loadVoice('en/en-us');
            meSpeak.speak('hello world');
    }
</script>

{% endblock %}
